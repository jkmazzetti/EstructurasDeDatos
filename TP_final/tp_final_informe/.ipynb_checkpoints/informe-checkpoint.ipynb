{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <h1><center>Estructuras de datos</center></h1>\n",
    "## <center>\"Extracción de datos de internet con Scrapy y Python\"</center>\n",
    "#### Universidad Nacional de Tres de Febrero, noviembre 2020.\n",
    "##### Jeremías Mazzetti\n",
    "##### Consultar documentación de Scrapy [aquí](https://docs.scrapy.org/en/latest/)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Módulos requeridos.</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import operator\n",
    "import string\n",
    "import unicodedata\n",
    "from abc import ABC, abstractmethod\n",
    "from datetime import datetime\n",
    "from scrapy.crawler import CrawlerProcess\n",
    "from scrapy.spiders import Rule,CrawlSpider\n",
    "from scrapy.linkextractors import LinkExtractor\n",
    "from nltk.corpus import stopwords\n",
    "from scrapy import Item, Field\n",
    "from scrapy.loader import ItemLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Campos de interés.</center>\n",
    "Se utilizó la clase Item incluida en Scrapy ya que permite constuir facilmente objetos de diccionarios que luego son utiles a la hora de hacer el yield y guardar en json o csv sin tener que importar módulos externos.\n",
    "Cada Field() representa la clave de cada diccionario contenido en el Item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Producto(Item):\n",
    "    supermercado = Field()\n",
    "    marca=Field()\n",
    "    producto=Field()\n",
    "    categoria=Field()\n",
    "    precio=Field()\n",
    "    link=Field()\n",
    "    hora=Field()\n",
    "    fecha_de_extraccion=Field()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Formato y remoción de stopwords.</center>\n",
    "Se utilizó el módulo nltk para quitar stopwords y se importó el módulo string incluido en Python para quitar caracteres indeseados, que podría arruinar la búsqueda. Éstas funciones se copiaron aquí porque de estas depende los resultados que se obtendran. Las mismas forman parte de los métodos de algunos de los Buscadores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def formatear(busqueda):\n",
    "    busqueda = ''.join((c for c in unicodedata.normalize('NFD', busqueda) if unicodedata.category(c) != 'Mn'))\n",
    "    for char in string.punctuation+\"¡¿\":\n",
    "        busqueda=busqueda.replace(char,'')\n",
    "    return busqueda.lower()\n",
    "\n",
    "def quitar_stopwords(busqueda):\n",
    "    palabras_busqueda = busqueda.split(' ')\n",
    "    busqueda_sin_stopwords = ''\n",
    "    for palabras in palabras_busqueda:\n",
    "        if palabras not in stopwords.words('spanish'):\n",
    "            busqueda_sin_stopwords += palabras + ' '\n",
    "    return busqueda_sin_stopwords[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center> Buscadores.</center>\n",
    "Para determinar como construir las clases del tipo Buscador se tuvo en cuanta que atributos y métodos son comunes a todos los tipos de Buscadores, cuales son abstractos y se deben definir en cada tipo de Buscador en particular.\n",
    "\n",
    "A continuación podrás apreciar la clase Buscador que es abstracta ya que contiene el método generar_url_reglas que debe estar contenido en las clases hijas pero que su comportamiento debe cambiar de acuerdo a el tipo de busqueda que el usuario requiere.La misma cuanta con los settings y método de ordenamiento que se utilizará luego de la extracción de datos, si el usuario así lo desea.\n",
    "\n",
    "El metodo generar archivo generar_archivo_ordenado_por_menor_precio contempla dos tipos de fallas, si el archivo que requiere leer no exite y si hay incompatibilidad entre los datos que se estan intentado ordenar.\n",
    "\n",
    "A modo de poder simplificar, se incluyen las clases requeridas y solo una spider"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Buscador(ABC):\n",
    "\n",
    "    def __init__(self, busqueda, formato):\n",
    "        self.busqueda = self.formatear(busqueda)\n",
    "        self.tipo_busqueda = ''\n",
    "        self.date = str(datetime.now().date())\n",
    "        self.ruta_destino = 'Salida/' + self.busqueda.replace(' ', '_') + '_' + self.date + '.' + formato\n",
    "        self.formato = formato\n",
    "        self.process = CrawlerProcess({\n",
    "            'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '\n",
    "                          'Chrome/86.0.4240.111 Safari/537.36',\n",
    "            'FEED_EXPORT_FIELDS': ['fecha_de_extraccion', 'precio', 'producto', 'marca', 'supermercado', 'categoria',\n",
    "                                   'link', 'hora'],\n",
    "            'FEED_EXPORT_ENCODING': 'utf-8',\n",
    "            'FEED_FORMAT': formato,\n",
    "            'FEED_URI': self.ruta_destino\n",
    "        })\n",
    "\n",
    "    def iniciar_busqueda(self):\n",
    "        urls, reglas = self.generar_url_reglas()\n",
    "        laamistad_start = urls[0]\n",
    "        laamistad_rules = reglas[0]\n",
    "        self.process.crawl(LaamistadSpider, start_urls=laamistad_start, rules=laamistad_rules)\n",
    "        self.process.start()\n",
    "\n",
    "    @abstractmethod\n",
    "    def generar_url_reglas(self):\n",
    "        pass\n",
    "\n",
    "    def generar_archivo_ordenado_por_menor_precio(self):\n",
    "        try:\n",
    "            if self.formato == 'csv':\n",
    "                with open(self.ruta_destino, 'r') as file:\n",
    "                    reader = csv.DictReader(file)\n",
    "                    unsorted_dict = {}\n",
    "                    for fila in reader:\n",
    "                        supermercado = fila['supermercado']\n",
    "                        marca = fila['marca']\n",
    "                        producto = fila['producto']\n",
    "                        categoria = fila['categoria']\n",
    "                        hora = fila['hora']\n",
    "                        precio = fila['precio']\n",
    "                        link = fila['link']\n",
    "                        fecha_de_extraccion = fila['fecha_de_extraccion']\n",
    "                        lista = [fecha_de_extraccion, producto, marca, supermercado, categoria, link, hora]\n",
    "                        precio = precio.replace(\" \", \"\").replace(\"$\", \"\").replace(\",\", \".\")\n",
    "                        if precio != 'precio' and precio != 'xkg' and precio != '' and precio.count('.') == 1:\n",
    "                            precio = float(precio)\n",
    "                            if precio not in unsorted_dict.keys():\n",
    "                                unsorted_dict[precio] = [lista]\n",
    "                            else:\n",
    "                                list_aux = []\n",
    "                                for lista_rec in unsorted_dict.get(precio):\n",
    "                                    list_aux.append(lista_rec)\n",
    "                                list_aux.append(lista)\n",
    "                                unsorted_dict[precio] = list_aux\n",
    "                    sorted_dict = dict(sorted(unsorted_dict.items(), key=operator.itemgetter(0)))\n",
    "                    with open(self.ruta_destino, 'w', newline=\"\") as archivo:\n",
    "                        campos_archivo = ['fecha_de_extraccion', 'precio', 'producto', 'marca', 'supermercado',\n",
    "                                          'categoria', 'link', 'hora']\n",
    "                        writer = csv.DictWriter(archivo, fieldnames=campos_archivo)\n",
    "                        writer.writeheader()\n",
    "                        for precio in sorted_dict.keys():\n",
    "                            for lista in sorted_dict.get(precio):\n",
    "                                writer.writerow({campos_archivo[0]: lista[0], campos_archivo[1]: precio,\n",
    "                                                 campos_archivo[2]: lista[1],\n",
    "                                                 campos_archivo[3]: lista[2], campos_archivo[4]: lista[3],\n",
    "                                                 campos_archivo[5]: lista[4],\n",
    "                                                 campos_archivo[6]: lista[5], campos_archivo[7]: lista[6]})\n",
    "        except FileNotFoundError as e:\n",
    "            print('No se encontró el archivo.')\n",
    "        except ValueError as e:\n",
    "            print('Hubo incompatibilidad datos.\\nPuede que la pagina haya cambiado.')\n",
    "\n",
    "    def formatear(self, busqueda):\n",
    "        busqueda = ''.join((c for c in unicodedata.normalize('NFD', busqueda) if unicodedata.category(c) != 'Mn'))\n",
    "        for char in string.punctuation + \"¡¿\":\n",
    "            busqueda = busqueda.replace(char, '')\n",
    "        return busqueda.lower()\n",
    "    \n",
    "class BuscadorVariable(Buscador):\n",
    "\n",
    "    def __init__(self, busqueda, formato):\n",
    "        super(BuscadorVariable, self).__init__(busqueda, formato)\n",
    "        self.tipo_busqueda = 'variable'\n",
    "\n",
    "    def generar_url_reglas(self):\n",
    "        LaamistadSpider.busqueda = self.quitar_stopwords(self.busqueda)\n",
    "        self.busqueda = self.quitar_stopwords(self.busqueda)\n",
    "        laamistad_start = []\n",
    "        laamistad_rules = [Rule(LinkExtractor(allow=r'page/')),\n",
    "                           Rule(LinkExtractor(allow=r'tienda/'), callback=self.tipo_busqueda)]\n",
    "        for palabra in self.busqueda.split(\" \"):\n",
    "            laamistad_start.append('https://www.tiendalaamistad.com.ar/?s=' + palabra)\n",
    "        urls = [laamistad_start]\n",
    "        reglas = [laamistad_rules]\n",
    "        return urls, reglas\n",
    "\n",
    "    def quitar_stopwords(self, busqueda):\n",
    "        palabras_busqueda = busqueda.split(' ')\n",
    "        busqueda_sin_stopwords = ''\n",
    "        for palabras in palabras_busqueda:\n",
    "            if palabras not in stopwords.words('spanish'):\n",
    "                busqueda_sin_stopwords += palabras + ' '\n",
    "        return busqueda_sin_stopwords[:-1]\n",
    "    \n",
    "class BuscadorTodo(BuscadorVariable):\n",
    "\n",
    "    def __init__(self, busqueda, formato):\n",
    "        super(BuscadorTodo, self).__init__(busqueda, formato)\n",
    "        self.tipo_busqueda = 'todo'\n",
    "\n",
    "    def generar_url_reglas(self):\n",
    "        LaamistadSpider.busqueda = self.quitar_stopwords(self.busqueda)\n",
    "        self.busqueda = self.busqueda.split(' ')[0]\n",
    "        laamistad_start = []\n",
    "        laamistad_rules = [Rule(LinkExtractor(allow=r'page/')),\n",
    "                           Rule(LinkExtractor(allow=r'tienda/'), callback=self.tipo_busqueda)]\n",
    "        laamistad_start.append('https://www.tiendalaamistad.com.ar/?s=' + self.busqueda)\n",
    "        urls = [laamistad_start]\n",
    "        reglas = [laamistad_rules]\n",
    "        return urls, reglas\n",
    "    \n",
    "class BuscadorExacto(Buscador):\n",
    "\n",
    "    def __init__(self, busqueda, formato):\n",
    "        super(BuscadorExacto, self).__init__(busqueda, formato)\n",
    "        self.tipo_busqueda='exacto'\n",
    "\n",
    "    def generar_url_reglas(self):\n",
    "        LaamistadSpider.busqueda = self.busqueda\n",
    "        laamistad_start = ['https://www.tiendalaamistad.com.ar/?s=' + self.busqueda.replace(' ', '+')]\n",
    "        laamistad_rules = [Rule(LinkExtractor(allow=r'page/')),\n",
    "                           Rule(LinkExtractor(allow=r'tienda/'), callback=self.tipo_busqueda)]\n",
    "        urls = [laamistad_start]\n",
    "        reglas = [laamistad_rules]\n",
    "        return urls, reglas\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>Spider.</center>\n",
    "En esta sección se encuentran uno de las spiders que se utilizaron para la extracción de datos de cada super. Parse es el método que recupera los datos y asigna valores a los distintos campos de Producto. Los métodos exacto, variable y todo son llamados luego de haber accedido a las urls que cumplen con las reglas dadas y si se cumple cada condicion solicitada realizan un yield donde recogen y almacenan los datos de la web.\n",
    "\n",
    "En el caso particular del spider con name 'laamistad' tiene dos reglas ya que la pagina carga los producto de forma vertical y horizontal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LaamistadSpider(CrawlSpider):\n",
    "    name = 'laamistad'\n",
    "    busqueda = ''\n",
    "    allowed_domains = ['tiendalaamistad.com.ar']\n",
    "\n",
    "    def formatear(self,busqueda):\n",
    "        busqueda = ''.join((c for c in unicodedata.normalize('NFD', busqueda) if unicodedata.category(c) != 'Mn'))\n",
    "        for char in string.punctuation + \"¡¿\":\n",
    "            busqueda = busqueda.replace(char, '')\n",
    "        return busqueda.lower()\n",
    "\n",
    "    def parse(self, response):\n",
    "        producto = ItemLoader(Producto(), response)\n",
    "        producto.add_value('supermercado', 'LA AMISTAD')\n",
    "        producto.add_xpath('producto',\n",
    "                           '/html/body/div[1]/div/div/div/main/div/div[1]/div[2]/div[1]/div/div[2]/div/h1/text()')\n",
    "        producto.add_xpath('categoria',\n",
    "                           '/html/body/div[1]/div/div/div/main/div/div[1]/div[2]/div[1]/div/div[2]/div/div[3]/span[2]/a[1]/text()')\n",
    "        precio = response.xpath(\n",
    "\n",
    "            '/html/body/div[1]/div/div/div/main/div/div[1]/div[2]/div[1]/div/div[2]/div/p[1]/span/text()').getall()[0]\n",
    "        precio = '$' + precio\n",
    "        producto.add_value('precio', precio)\n",
    "        producto.add_value('link', response.url)\n",
    "        producto.add_css('marca', '.woocommerce-product-attributes-item--attribute_pa_marca a::text')\n",
    "        time = str(datetime.now().time().isoformat('seconds'))\n",
    "        date = str(datetime.now().date())\n",
    "        producto.add_value('fecha_de_extraccion', date)\n",
    "        producto.add_value('hora', time)\n",
    "        comparar = response.xpath(\n",
    "            '/html/body/div[1]/div/div/div/main/div/div[1]/div[2]/div[1]/div/div[2]/div/h1/text()').getall()[0]\n",
    "        comparar=self.formatear(comparar)\n",
    "        return producto.load_item(), comparar\n",
    "\n",
    "    def exacto(self, response):\n",
    "        producto = self.parse(response)[0]\n",
    "        resultado = self.parse(response)[1]\n",
    "        if self.busqueda == resultado:\n",
    "            yield producto\n",
    "\n",
    "    def variable(self, response):\n",
    "        producto = self.parse(response)[0]\n",
    "        resultado = self.parse(response)[1].split(' ')\n",
    "        contiene_alguna_palabra = False\n",
    "        for palabra_busqueda in self.busqueda.split(' '):\n",
    "            for palabra_resultado in resultado:\n",
    "                if palabra_busqueda == palabra_resultado:\n",
    "                    contiene_alguna_palabra = True\n",
    "                    break\n",
    "            if contiene_alguna_palabra:\n",
    "                break\n",
    "        if contiene_alguna_palabra:\n",
    "            yield producto\n",
    "\n",
    "    def todo(self, response):\n",
    "        producto = self.parse(response)[0]\n",
    "        resultado = self.parse(response)[1].split(\" \")\n",
    "        contiene_todo = True\n",
    "        for palabra in self.busqueda.split(\" \"):\n",
    "            if not resultado.__contains__(palabra):\n",
    "                contiene_todo = False\n",
    "                break\n",
    "        if contiene_todo:\n",
    "            yield producto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <center>\"Interfaz\" de usuario.</center>\n",
    "Aquí es donde se ejecuta el programa, solicitará ingresar una busqueda, determinar un formato y escribir un archivo ordenado en formato .csv\n",
    "Si la búsqueda está vacía, no podrá buscar mas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "¿Qué buscamos?\n",
      "harina integral\n",
      "¿En qué formato?\n",
      "1.CSV ordenado de menor a mayor.\n",
      "2.JSON crudo.\n",
      "1\n",
      "¿Qué tipo de busqueda?\n",
      "1.Publicación con la frase exacta\n",
      "2.Publicación que contenga todas las palabras\n",
      "3.Publicación que contenga algunas de las palabras\n",
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-03 11:32:26 [scrapy.utils.log] INFO: Scrapy 2.4.0 started (bot: scrapybot)\n",
      "2020-12-03 11:32:26 [scrapy.utils.log] INFO: Versions: lxml 4.6.1.0, libxml2 2.9.10, cssselect 1.1.0, parsel 1.6.0, w3lib 1.22.0, Twisted 20.3.0, Python 3.8.5 (default, Jul 28 2020, 12:59:40) - [GCC 9.3.0], pyOpenSSL 19.1.0 (OpenSSL 1.1.1f  31 Mar 2020), cryptography 2.8, Platform Linux-5.4.0-54-generic-x86_64-with-glibc2.29\n",
      "2020-12-03 11:32:26 [scrapy.utils.log] DEBUG: Using reactor: twisted.internet.epollreactor.EPollReactor\n",
      "2020-12-03 11:32:26 [scrapy.crawler] INFO: Overridden settings:\n",
      "{'FEED_EXPORT_ENCODING': 'utf-8',\n",
      " 'FEED_EXPORT_FIELDS': ['fecha_de_extraccion',\n",
      "                        'precio',\n",
      "                        'producto',\n",
      "                        'marca',\n",
      "                        'supermercado',\n",
      "                        'categoria',\n",
      "                        'link',\n",
      "                        'hora'],\n",
      " 'USER_AGENT': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 '\n",
      "               '(KHTML, like Gecko) Chrome/86.0.4240.111 Safari/537.36'}\n",
      "2020-12-03 11:32:26 [scrapy.extensions.telnet] INFO: Telnet Password: b46e663345b48077\n",
      "2020-12-03 11:32:26 [py.warnings] WARNING: /home/ubuntu/.local/lib/python3.8/site-packages/scrapy/extensions/feedexport.py:247: ScrapyDeprecationWarning: The `FEED_URI` and `FEED_FORMAT` settings have been deprecated in favor of the `FEEDS` setting. Please see the `FEEDS` setting docs for more details\n",
      "  exporter = cls(crawler)\n",
      "\n",
      "2020-12-03 11:32:26 [scrapy.middleware] INFO: Enabled extensions:\n",
      "['scrapy.extensions.corestats.CoreStats',\n",
      " 'scrapy.extensions.telnet.TelnetConsole',\n",
      " 'scrapy.extensions.memusage.MemoryUsage',\n",
      " 'scrapy.extensions.feedexport.FeedExporter',\n",
      " 'scrapy.extensions.logstats.LogStats']\n",
      "2020-12-03 11:32:26 [scrapy.middleware] INFO: Enabled downloader middlewares:\n",
      "['scrapy.downloadermiddlewares.httpauth.HttpAuthMiddleware',\n",
      " 'scrapy.downloadermiddlewares.downloadtimeout.DownloadTimeoutMiddleware',\n",
      " 'scrapy.downloadermiddlewares.defaultheaders.DefaultHeadersMiddleware',\n",
      " 'scrapy.downloadermiddlewares.useragent.UserAgentMiddleware',\n",
      " 'scrapy.downloadermiddlewares.retry.RetryMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.MetaRefreshMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpcompression.HttpCompressionMiddleware',\n",
      " 'scrapy.downloadermiddlewares.redirect.RedirectMiddleware',\n",
      " 'scrapy.downloadermiddlewares.cookies.CookiesMiddleware',\n",
      " 'scrapy.downloadermiddlewares.httpproxy.HttpProxyMiddleware',\n",
      " 'scrapy.downloadermiddlewares.stats.DownloaderStats']\n",
      "2020-12-03 11:32:26 [scrapy.middleware] INFO: Enabled spider middlewares:\n",
      "['scrapy.spidermiddlewares.httperror.HttpErrorMiddleware',\n",
      " 'scrapy.spidermiddlewares.offsite.OffsiteMiddleware',\n",
      " 'scrapy.spidermiddlewares.referer.RefererMiddleware',\n",
      " 'scrapy.spidermiddlewares.urllength.UrlLengthMiddleware',\n",
      " 'scrapy.spidermiddlewares.depth.DepthMiddleware']\n",
      "2020-12-03 11:32:26 [scrapy.middleware] INFO: Enabled item pipelines:\n",
      "[]\n",
      "2020-12-03 11:32:26 [scrapy.core.engine] INFO: Spider opened\n",
      "2020-12-03 11:32:26 [scrapy.extensions.logstats] INFO: Crawled 0 pages (at 0 pages/min), scraped 0 items (at 0 items/min)\n",
      "2020-12-03 11:32:26 [scrapy.extensions.telnet] INFO: Telnet console listening on 127.0.0.1:6023\n",
      "2020-12-03 11:32:29 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://www.tiendalaamistad.com.ar/?s=harina> (referer: None)\n",
      "2020-12-03 11:32:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/page/2/?s=harina> (referer: https://www.tiendalaamistad.com.ar/?s=harina)\n",
      "2020-12-03 11:32:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/almacen/harinas/harina-de-arroz/natuzen-harina-de-arroz-1-kg/> (referer: https://www.tiendalaamistad.com.ar/?s=harina)\n",
      "2020-12-03 11:32:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/almacen/harinas/premezcla/pureza-harina-para-pizza-1-kg/> (referer: https://www.tiendalaamistad.com.ar/?s=harina)\n",
      "2020-12-03 11:32:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/almacen/harinas/harina-leudante/pureza-harina-leudante-sin-sal-1-kg/> (referer: https://www.tiendalaamistad.com.ar/?s=harina)\n",
      "2020-12-03 11:32:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/almacen/harinas/harina-0000/pureza-harina-0000-1-kg/> (referer: https://www.tiendalaamistad.com.ar/?s=harina)\n",
      "2020-12-03 11:32:31 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/almacen/harinas/harina-000/favorita-harina-000-1-kg/> (referer: https://www.tiendalaamistad.com.ar/?s=harina)\n",
      "2020-12-03 11:32:32 [scrapy.dupefilters] DEBUG: Filtered duplicate request: <GET https://tiendalaamistad.com.ar/page/2/?s=harina> - no more duplicates will be shown (see DUPEFILTER_DEBUG to show all duplicates)\n",
      "2020-12-03 11:32:32 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/?swoof=1&pa_categoriaadicional=mayorista> (referer: https://www.tiendalaamistad.com.ar/?s=harina)\n",
      "2020-12-03 11:32:33 [scrapy.core.scraper] ERROR: Spider error processing <GET https://tiendalaamistad.com.ar/tienda/?swoof=1&pa_categoriaadicional=mayorista> (referer: https://www.tiendalaamistad.com.ar/?s=harina)\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/utils/defer.py\", line 120, in iter_errback\n",
      "    yield next(it)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/utils/python.py\", line 353, in __next__\n",
      "    return next(self.data)\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/spidermiddlewares/offsite.py\", line 29, in process_spider_output\n",
      "    for x in result:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/spidermiddlewares/referer.py\", line 340, in <genexpr>\n",
      "    return (_set_referer(r) for r in result or ())\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/spidermiddlewares/urllength.py\", line 37, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/spidermiddlewares/depth.py\", line 58, in <genexpr>\n",
      "    return (r for r in result or () if _filter(r))\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/core/spidermw.py\", line 62, in _evaluate_iterable\n",
      "    for r in iterable:\n",
      "  File \"/home/ubuntu/.local/lib/python3.8/site-packages/scrapy/spiders/crawl.py\", line 116, in _parse_response\n",
      "    for request_or_item in iterate_spider_output(cb_res):\n",
      "  File \"<ipython-input-5-f90cf6d77250>\", line 56, in todo\n",
      "    producto = self.parse(response)[0]\n",
      "  File \"<ipython-input-5-f90cf6d77250>\", line 19, in parse\n",
      "    precio = response.xpath(\n",
      "IndexError: list index out of range\n",
      "2020-12-03 11:32:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/almacen/harinas/harina-000/pureza-harina-000-1kg/> (referer: https://www.tiendalaamistad.com.ar/?s=harina)\n",
      "2020-12-03 11:32:33 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/almacen/harinas/harina-de-maiz/lisetta-harina-de-maiz-500-gr/> (referer: https://tiendalaamistad.com.ar/page/2/?s=harina)\n",
      "2020-12-03 11:32:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/page/3/?s=harina> (referer: https://www.tiendalaamistad.com.ar/?s=harina)\n",
      "2020-12-03 11:32:34 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/desayuno-y-merienda/galletitas/dulces-secas/cachafaz-225-gr-gall-harina-integral-y-algar/> (referer: https://tiendalaamistad.com.ar/page/2/?s=harina)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-03 11:32:34 [scrapy.core.scraper] DEBUG: Scraped from <200 https://tiendalaamistad.com.ar/tienda/desayuno-y-merienda/galletitas/dulces-secas/cachafaz-225-gr-gall-harina-integral-y-algar/>\n",
      "{'categoria': ['- DULCES SECAS'],\n",
      " 'fecha_de_extraccion': ['2020-12-03'],\n",
      " 'hora': ['11:32:34'],\n",
      " 'link': ['https://tiendalaamistad.com.ar/tienda/desayuno-y-merienda/galletitas/dulces-secas/cachafaz-225-gr-gall-harina-integral-y-algar/'],\n",
      " 'marca': ['CACHAFAZ'],\n",
      " 'precio': ['$139.99'],\n",
      " 'producto': ['CACHAFAZ 225 GR GALL HARINA INTEGRAL Y ALGAR'],\n",
      " 'supermercado': ['LA AMISTAD']}\n",
      "2020-12-03 11:32:35 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/almacen/harinas/harina-leudante/pureza-harina-leudante-1-kg/> (referer: https://tiendalaamistad.com.ar/page/2/?s=harina)\n",
      "2020-12-03 11:32:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/almacen/harinas/harina-leudante/blancaflor-harina-leudante-n-1-kg/> (referer: https://tiendalaamistad.com.ar/page/2/?s=harina)\n",
      "2020-12-03 11:32:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/almacen/harinas/harina-integral/pureza-harina-integral-1-kg/> (referer: https://tiendalaamistad.com.ar/page/3/?s=harina)\n",
      "2020-12-03 11:32:36 [scrapy.core.scraper] DEBUG: Scraped from <200 https://tiendalaamistad.com.ar/tienda/almacen/harinas/harina-integral/pureza-harina-integral-1-kg/>\n",
      "{'categoria': ['- HARINA INTEGRAL'],\n",
      " 'fecha_de_extraccion': ['2020-12-03'],\n",
      " 'hora': ['11:32:36'],\n",
      " 'link': ['https://tiendalaamistad.com.ar/tienda/almacen/harinas/harina-integral/pureza-harina-integral-1-kg/'],\n",
      " 'marca': ['PUREZA'],\n",
      " 'precio': ['$74.99'],\n",
      " 'producto': ['PUREZA HARINA INTEGRAL 1 KG'],\n",
      " 'supermercado': ['LA AMISTAD']}\n",
      "2020-12-03 11:32:36 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/desayuno-y-merienda/galletitas/dulces-secas/canvas-cookies-vainilla-con-harina-integral-150-gr/> (referer: https://tiendalaamistad.com.ar/page/2/?s=harina)\n",
      "2020-12-03 11:32:37 [scrapy.core.scraper] DEBUG: Scraped from <200 https://tiendalaamistad.com.ar/tienda/desayuno-y-merienda/galletitas/dulces-secas/canvas-cookies-vainilla-con-harina-integral-150-gr/>\n",
      "{'categoria': ['- DULCES SECAS'],\n",
      " 'fecha_de_extraccion': ['2020-12-03'],\n",
      " 'hora': ['11:32:37'],\n",
      " 'link': ['https://tiendalaamistad.com.ar/tienda/desayuno-y-merienda/galletitas/dulces-secas/canvas-cookies-vainilla-con-harina-integral-150-gr/'],\n",
      " 'marca': ['CANVAS'],\n",
      " 'precio': ['$72.99'],\n",
      " 'producto': ['CANVAS COOKIES VAINILLA CON HARINA INTEGRAL 150 GR'],\n",
      " 'supermercado': ['LA AMISTAD']}\n",
      "2020-12-03 11:32:37 [scrapy.core.engine] DEBUG: Crawled (200) <GET https://tiendalaamistad.com.ar/tienda/almacen/aceites-acetos-grasas-y-aceto/aerosol/canuelas-rocio-150-gr-manteca-harina/> (referer: https://tiendalaamistad.com.ar/page/2/?s=harina)\n",
      "2020-12-03 11:32:37 [scrapy.core.engine] INFO: Closing spider (finished)\n",
      "2020-12-03 11:32:37 [scrapy.extensions.feedexport] INFO: Stored csv feed (3 items) in: Salida/harina_integral_2020-12-03.csv\n",
      "2020-12-03 11:32:37 [scrapy.statscollectors] INFO: Dumping Scrapy stats:\n",
      "{'downloader/request_bytes': 8590,\n",
      " 'downloader/request_count': 17,\n",
      " 'downloader/request_method_count/GET': 17,\n",
      " 'downloader/response_bytes': 888947,\n",
      " 'downloader/response_count': 17,\n",
      " 'downloader/response_status_count/200': 17,\n",
      " 'dupefilter/filtered': 6,\n",
      " 'elapsed_time_seconds': 10.958211,\n",
      " 'finish_reason': 'finished',\n",
      " 'finish_time': datetime.datetime(2020, 12, 3, 14, 32, 37, 731799),\n",
      " 'item_scraped_count': 3,\n",
      " 'log_count/DEBUG': 21,\n",
      " 'log_count/ERROR': 1,\n",
      " 'log_count/INFO': 11,\n",
      " 'log_count/WARNING': 1,\n",
      " 'memusage/max': 105046016,\n",
      " 'memusage/startup': 105046016,\n",
      " 'request_depth_max': 2,\n",
      " 'response_received_count': 17,\n",
      " 'scheduler/dequeued': 17,\n",
      " 'scheduler/dequeued/memory': 17,\n",
      " 'scheduler/enqueued': 17,\n",
      " 'scheduler/enqueued/memory': 17,\n",
      " 'spider_exceptions/IndexError': 1,\n",
      " 'start_time': datetime.datetime(2020, 12, 3, 14, 32, 26, 773588)}\n",
      "2020-12-03 11:32:37 [scrapy.core.engine] INFO: Spider closed (finished)\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    aceptado = False\n",
    "    print('¿Qué buscamos?')\n",
    "    busqueda = BuscadorVariable.formatear(BuscadorVariable,str(input()))\n",
    "    LaamistadSpider.busqueda = busqueda\n",
    "    if busqueda!='':\n",
    "        while not aceptado:\n",
    "            print('¿En qué formato?\\n1.CSV ordenado de menor a mayor.\\n2.JSON crudo.')\n",
    "            seleccion = int(input())\n",
    "            if seleccion == 1:\n",
    "                formato = 'csv'\n",
    "                aceptado = True\n",
    "            elif seleccion == 2:\n",
    "                formato = 'json'\n",
    "                aceptado = True\n",
    "        aceptado = False\n",
    "        while not aceptado:\n",
    "            print('¿Qué tipo de busqueda?\\n1.Publicación con la frase exacta'\n",
    "                  '\\n2.Publicación que contenga todas las palabras'\n",
    "                  '\\n3.Publicación que contenga algunas de las palabras')\n",
    "            seleccion = int(input())\n",
    "            if seleccion == 1:\n",
    "                aceptado = True\n",
    "                nuevo_buscador=BuscadorExacto(busqueda,formato)\n",
    "                nuevo_buscador.iniciar_busqueda()\n",
    "                if formato=='csv':\n",
    "                    nuevo_buscador.generar_archivo_ordenado_por_menor_precio()\n",
    "            elif seleccion == 3:\n",
    "                aceptado = True\n",
    "                nuevo_buscador=BuscadorVariable(busqueda,formato)\n",
    "                nuevo_buscador.iniciar_busqueda()\n",
    "                if formato == 'csv':\n",
    "                    nuevo_buscador.generar_archivo_ordenado_por_menor_precio()\n",
    "            elif seleccion == 2:\n",
    "                aceptado = True\n",
    "                nuevo_buscador=BuscadorTodo(busqueda,formato)\n",
    "                nuevo_buscador.iniciar_busqueda()\n",
    "                if formato == 'csv':\n",
    "                    nuevo_buscador.generar_archivo_ordenado_por_menor_precio()\n",
    "    else:\n",
    "        print('No buscamos nada, chau!')\n",
    "                \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
